%!TEX root = main.tex
\titleformat{\section}{\normalfont\large\bfseries}{\thesection}{1em}{}
\renewcommand{\chaptername}{Project}
\renewcommand{\thesection}{\arabic{section}}


\chapter{Linear Prediction of Speech}
\begin{center}
Guangting Yu, University of Michigan - Shanghai Jiao Tong University Joint Institute, Minhang, Shanghai, 200135, China
\end{center}


\section*{Abstract}
This paper finds an efficient algorithm to predict the speech signal based on the previously recorded speech signal.
The prediction is linear in that the prediction signal can be written as a linear combination of previous signals, while the coefficients are the core of the prediction.
Obtaining the best coefficients is equivalent to solving a system of linear equations, where Cholesky decomposition is applied to achieve the goal.
The pseudocode and MATLAB\texttrademark\ code of the Cholesky decomposition are implemented.


\section{Introductory Background}
Speech production is the result of an excitation signal generated by the contraction of the lungs when they expel air.
It is then modified by resonances when passing through the trachea, the vocal cords, the mouth cavity, as well as various muscles.
The excitation signal is either created by the opening and closing of the vocal cords, or by a continuous flow of air.
Introduced in the early 1960s by Fant, \textit{the source-filter} model assumes that the glottis and vocal tract are fully uncoupled.
This initial idea was reused to develop the \textit{Linear Predictive} (LP) model for speech production.\cite{dutoit}

In this model the speech signal is the output \(y[n]\) of an \textit{all-pole filter}\footnote{A filter whose frequency response function goes infinite at specific frequencies} \(1/A(z)\) excited by \(x[n]\).
Calling \(Y(z)\) and \(X(z)\) the Z-transform of the speech and excitation, respectively, the model is described by
\begin{equation}
Y(z)=\frac{X(z)}{1-\sum_{i=0}^p a_i z^{-i}}=\frac{X(z)}{A_p}.
\end{equation}

Applying the inverse Z-transform to this equation we observe that the speech can be linearly predicted from the previous $p$ samples and some excitation:
\begin{equation}\label{model}
y[n] = x[n]+\sum_{i=1}^p a_i y[n-i].
\end{equation}\cite{cc12}


\section{Model Construction and Simplification}
Our goal is to explain as much as possible of \(y[n]\) through the $a_i$ , \textit{i.e.}, we look at \(x[n]\) as an error, and we strive at rendering it as small and simple as possible.
For the sake of clarity we therefore rename \(x[n]\) into \(e[n]\).
The question we want to answer is how to select the $a_i$ such as to minimize the energy
\begin{equation}
E=\sum_{m=-\infty}^\infty e^2[m].
\end{equation}
Plug in equation (\ref{model}), we have the error expressed by an infinite sum
\[ E= \sum_{m=-\infty}^\infty \left(y[m]-\sum_{i=1}^p a_i y[m-i] \right)^2. \]
However, infinite sum cannot be computed, and it needs to be truncated.
This can be achieved by applying the covariance method, which consists in windowing the error
\begin{equation}
E_n=\sum_{m=n}^{n+N-1}\left(y[m]-\sum_{i=1}^p a_i y[m-i] \right)^2.
\end{equation}
Furthermore, we observe the recursive relationship between these terms
\begin{equation}\label{recursive}
	\sum_{m=-\infty}^{\infty} y[m]y[m-k]=\sum_{m=-\infty}^{\infty} \left(x[m]+ \sum_{i=1}^p a_iy[m-i]\right)y[m-k], \quad k=1,\cdots,p.
\end{equation}
We want to make the prediction unbiased, so the error should have zero mean.
\[ \sum_{m=-\infty}^{+\infty}x[m]=0. \]
Furthermorre, we also want the error be independent to the signal
\[ \sum_{m=-\infty}^{+\infty}x[m]y[m-k]=0. \]
Then, equation (\ref{recursive}) can be simplified to
\begin{equation}
\sum_{m=-\infty}^{\infty} y[m]y[m-k]=\sum_{i=1}^p a_i\sum_{m=-\infty}^{\infty} y[m-i]y[m-k], \quad k=1,\cdots,p.
\end{equation}
Again, we apply the truncation to avoid infinite calculation,
\begin{equation}\label{simple}
\sum_{m=n}^{n+N-1} y[m]y[m-k]=\sum_{i=1}^p a_i\sum_{m=n}^{n+N-1} y[m-i]y[m-k], \quad k=1,\cdots,p.
\end{equation}
Define
\begin{equation}
\phi_n(k,i)=\sum_{m=n}^{n+N-1} y[m-k]y[m-i].
\end{equation}
Then, equation (\ref{simple}) can be written in
\begin{equation}
\phi_n(k,0)=\sum_{i=0}^p a_i\phi_n(k,i), \quad  k=1,\cdots,p.
\end{equation}
This can be further written in the form of linear equation
\begin{equation}
	\begin{pmatrix} \phi(1,0)\\ \vdots \\ \phi(p,0) \end{pmatrix}=\begin{pmatrix} \phi(1,1) & \cdots & \phi(1,p) \\ \vdots & & \vdots \\ \phi(p,1) & \cdots & \phi(p,p) \end{pmatrix}\begin{pmatrix} a_1 \\ \vdots \\ a_p \end{pmatrix}.
\end{equation}
Solving for \(a_i, 1\leq i\leq p\) requires to invert the matrix $\Phi$.
This can be achieved through Gauss elimination.
However, we will use Cholesky decomposition \ref{cholesky} in this paper instead.


\section{Mathematical Basis}


\subsection{Linear Algebra}
\begin{definition}[Hermitian matrix]
A Matrix \(A\in\C^{n\times n}\) is \emph{Hermitian} if it is equal to the conjugate transpose of itself, \textit{i.e.},
\begin{equation*}
A=\bar{A}^T.
\end{equation*}
\end{definition}


\begin{definition}[Eigenvalues ad eigenvectors]
A number \(\lambda\in\C\) is called an \emph{eigenvalue} of \(A\in\C^{n\times n}\) if there exists \(v\in\C^n\) such that \(v\neq0\) and
\begin{equation*}
Av=\lambda v.
\end{equation*}
Such a vector $v$ is called an \emph{eigenvector}.
\end{definition}


\begin{definition}[Spectrum]
The \emph{spectrum} of \(A\in\C^{n\times n}\), denoted as \(\sigma_A\), is the set of all the eigenvalues of $A$, \textit{i.e.},
\begin{equation*}
\sigma_A\coloneqq\Set{\lambda}{\lambda \text{ is a eigenvalue of }A}
\end{equation*}
\end{definition}


\begin{definition}[Spectrum radius]
The \emph{spectrum radius} of \(A\in\C^{n\times n}\), denoted as \(\rho(A)\), is the largest absolute value of all its eigenvalues, \textit{i.e.},
\begin{equation*}
\rho(A)\coloneqq\max_{\lambda\in\sigma_A}|\lambda|.
\end{equation*}
\end{definition}


\begin{theorem}
If a matrix \(A\in\C^{n\times n}\) is Hermitian, then \(\sigma_A\subset\R\).
\end{theorem}
\begin{proof}
Take any eigenvalue \(\lambda\in\sigma_A\) and its corresponding eigenvector \(v\in\C^n\).
By definition,
\begin{equation*}
Av=\lambda v.
\end{equation*}
Multiply both sides by the conjugate transpose of $v$,
\begin{equation}\label{hermitianproof}
\bar{v}^T Av=\lambda \bar{v}^T v.
\end{equation}
The left hand side of the equation has the conjugate transpose
\begin{equation*}
\overline{\bar{v}^T Av}^T=\bar{v}^T\bar{A}^T v.
\end{equation*}
Since $A$ is Hermitian, we plug in \(\bar{A}^T=A\) on the right hand side and
\begin{equation*}
\overline{\bar{v}^T Av}^T=\bar{v}^TA v,
\end{equation*}
which implies that \(\bar{v}^T Av\) is also Hermitian.
Furthermore, \(\bar{v}^T Av\in\C^{1\times1}\) implies that it is a real number.
Then, according to equation (\ref{hermitianproof})
\begin{equation*}
\lambda=\frac{\bar{v}^T Av}{\bar{v}^T v}.
\end{equation*}
The denominator is the inner product of $v$ with itself defined on \(\C^n\), so \(\bar{v}^T v\in\R_*^+\) since \(v\neq0\).
Both the numerator and the denominator are real, so the quotient $\lambda$ is also real.
Since $\lambda$ is arbitrarily selected from \(\sigma_A\), all elements in \(\sigma_A\) are real.
Furthermore, \(\sigma_A\) has at most $n$ elements, so it is a proper subset of \(\R\).
\end{proof}

\begin{theorem}
If \(A,B\in\C^{n\times n}\), then \(\sigma_{AB}=\sigma_{BA}\).
\end{theorem}
\begin{proof}
Select \(\lambda\in\sigma_{AB}\).
\item If \(\lambda=0\), then there exists \(v\in\C^n,v\neq0\) such that \(ABv=\lambda v=0\).
So the matrix $AB$ is not full rank, and thus \(\det(AB)=0\).
Then \(\det(BA)=\det(AB)=0\), which implies $BA$ is not full rank either.
Thus, there exists a nonzero vector \(u\in\C^n\) such that \(BAu=0\).
This means 0 is also an eigenvalue of $BA$.
\item If \(\lambda\neq0\), then there exists \(v\in\C^n,v\neq0\) such that \(ABv=\lambda v\).
Then, multiply both sides by $B$,
\begin{equation*}
BABv=\lambda Bv.
\end{equation*}
This also means \(BA\) has $\lambda$ as its eigenvalue with corresponding eigenvector \(Bv\).
Combine both cases, \(\lambda\in\sigma_{BA}\).
Since $\lambda$ is arbitrarily selected from \(\sigma_{AB}\), we have \(\sigma_{AB}\subseteq\sigma_{BA}\).
Exchange the name of $A$ and $B$, we also have \(\sigma_{AB}\supseteq\sigma_{BA}\).
Therefore, \(\sigma_{AB}=\sigma_{BA}\).
This directly implies \(\rho(AB)=\rho(BA)\).
\end{proof}

\begin{definition}[Positive definite]
Let \(A\in\R^{n\times n}\) be a symmetrix matrix, \textit{i.e.}, \(A=A^T\).
It is \emph{positive definite} if and only if
\[ \forall x\in\R^n,x\neq0 \qquad x^TAx>0.  \]
\end{definition}

\begin{theorem}
Let \(A\in\R^{n\times n}\) be a symmetrix matrix, \textit{i.e.}, \(A=A^T\).
Then $A$ is positive definite if and only if \(\sigma_A\subset\R_*^+\), \textit{i.e.}, all its eigenvalues are positive.
\end{theorem}
\begin{proof}
Select \(\lambda\in\sigma_{A}\) and its corresponding eigenvector \(v\in\R^n,v\neq0\).
The equation (\ref{hermitianproof}) for \(\R^{n\times n}\) deduces
\[ \lambda=\frac{v^T Av}{v^Tv}. \]
Since \(v\neq0\), its inner product of itself \(v^Tv>0\).
Then,
\[ v^TAv>0 \iff \lambda=\frac{v^TAv}{v^Tv}>0. \]
\end{proof}


\begin{definition}[Lower triangular matrix]
A matrix \(A\in\C^{n\times n}\) is \emph{lower triangular} if all the elements above its main diagonal are zero, \textit{i.e.},
\[ \forall a_{ij}\in A, i<j\implies a_{ij}=0. \]
\end{definition}


\begin{theorem}[Determinant of lower triangular matrix]\label{lowertriangle}
The determinant of a lower triangular matrix \(A\in\C^{n\times n}\) is the product of its main diagonal elements, \textit{i.e.},
\[ \det(A)=\prod_{i=1}^n a_{ii}. \]
\end{theorem}
\begin{proof}
This is a direct result from cofactor expansion of determinant.
And Gauss elimination also gives the same result.
\end{proof}


\subsection{System of Linear Equations}
The system of linear equations
\begin{equation}
\begin{cases}
a_{11}x_1+\cdots+a_{1n}x_n=b_1\\
\qquad\qquad\vdots \\
a_{n1}x_1+\cdots+a_{nn}x_n=b_n
\end{cases}
\end{equation}
can be written in matrix form
\begin{equation}
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn} \\
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{pmatrix}
=
\begin{pmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{pmatrix}
\end{equation}
with a simplified experssion
\[ AX=B. \]

\begin{theorem}
The solution to the system of linear equations 
\[ AX=B \]
where \(A\in\R^{n\times n}, X,B\in\R^n\) is unique if nad only if $A$ is invertible.
\end{theorem}
\begin{proof}
If $A$ is invertible, we can find $X$ by multiplying both sides by its inverse,
\[ A^{-1}AX=A^{-1}B \implies X=A^{-1}B, \]
which is unique.
If $A$ is not invertible, then the kernel (null space) of $A$ has nonzero dimension, and its range is proper subspace of \(\R^n\).
\begin{itemize}
	\item \(B\in\text{range}(A)\), then the solution exists by definition, but it is not unique.
	If $X$ is a solution to the system of equations, then we can select a nontrivial element \(v\in\text{null}(A)\) with \(v\neq0\) and a nontrivial scalar \(c\in\R,c\neq0\) so that
	\[ A(X+cv)=AX+cAv=AX+0=B, \]
	which means \(X+cv\) is also a solution, and uniqness is violated.
	\item \(B\notin\text{range}(A)\), then the solution doesn't exist by definition.
\end{itemize}
\end{proof}


\begin{theorem}[Cholesky decomposition]
\label{cholesky}
Let \(A\in\R^{n\times n}\) be a symmetric and positive definite matrix.
Then, there exists a unique lower triangular matrix $L$ with positive diagonal entries such that
\[ A=LL^T, \]
where the entries \(l_{ij}\) can be computed as follows: \(l_{11}=\sqrt{a_{11}}\), and for \(i=2\cdots n\),
\begin{align}
l_{ij}&=\frac{a_{ij}-\sum_{k=1}^{j-1}l_{ik}l_{jk}}{l_{jj}}, \quad j=1\cdots,i-1, \label{cho1}\\
l_{ii}&=\sqrt{a_{ii}-\sum_{k=1}^{i-1}l_{ik}^2} \label{cho2}.
\end{align}
\end{theorem}
\begin{proof}
We prove this by induction on the dimension of the matrix $A$.
For \(n=1\), the result is trivially true, since a positive scalar has a positive square root.
Assume this statement holds for \(n=k\), then there exists an lower triangular matrix \(L_k\) such that
\[ A_k=L_k L_k^T. \]
Since \(A_{k+1}\) is symmetric, we can partition it as
\[ A_{k+1}=\begin{pmatrix} A_k & v \\ v^T & \alpha \end{pmatrix} \]
with \(\alpha\in\R_*^{+},v\in\R^k\).
We need a lower trianglar factorization in the form
\[ A_{k+1}=L_{k+1}L_{k+1}^T=\begin{pmatrix} L_k & 0 \\  l^T & \beta \end{pmatrix}\begin{pmatrix} L_k^T & l \\  0 & \beta \end{pmatrix}. \]
Then we get two equations to satisfy
\begin{align*}
	L_k l&=v,\\
	l^T l+\beta^2&=\alpha,
\end{align*}
which directly gives the solution
\begin{align}
l&=L_k^{-1}v, \label{cho3}\\
\beta&=\sqrt{\alpha-l^Tl} \label{cho4}.
\end{align}
Here, we also need to prove that \(L_k\) is invertible and \(\beta\in\R_*^+\).
According to Theorem \ref{lowertriangle}, we know that the determinant of \(L_k\) is the product of its diagonal elements, and according to the induction hypothesis, they are all positive.
So the product is also positive, which means \(L_k\) is invertible, and thus (\ref{cho3}) gives a well defined vector.
Furthermore, the determinant of the product of two matrices is the product of the determinants of these two matrices, \textit{i.e.},
\[ \det(A_{k+1})=\det(L_{k+1})\cdot\det(L_{k+1}^T)=\beta^2\det(L_k)^2>0, \]
where the positiveness of \(\det(A_{k+1})\) comes from the induction hypothesis that it is positive definite, so that its determinant is the product of all its eigenvalues.
With \(\det(L_k)>0\), we obtain \(\beta^2>0\), which proves that (\ref{cho4}) defines a positive real number.
So the existence of the Cholesky decomposition is already proved, and it is easy to observe that (\ref{cho2}) is equivalent to (\ref{cho4}), and (\ref{cho1}) is equivalent to (\ref{cho3}).
\cite{tam37}
\end{proof}



\subsection{Functional Analysis}
\begin{definition}[Norm]
Given a vector space $V$ over a subfield \(\mathbb{F}\subseteq\C\), a \emph{norm} on $V$ is a function \(||\cdot||:V\to\R\) with the following properties:
\item For all \(c\in\mathbb{F}\) and all \(u,v\in V\),
\begin{enumerate}
	\item \(||v||\geq0\); \hfill Nonnegative
	\item \(||v||=0\implies v=0\); \hfill Positive
	\item \(||cv||=|c|\cdot||v||\); \hfill Homogeneous
	\item \(||u+v||\leq||u||+||v||\). \hfill Subadditive
\end{enumerate}
The subadditivity is also referred to as the triangle inequality.
\end{definition}


\begin{definition}[Matrix norm]
A \emph{matrix norm} is a function \(||\cdot||:\C^{n\times n}\to\R\) if for all \(A,B\in\C^{n\times n}\), it satisfies the following five properties.
\begin{enumerate}
	\item \(||A||\geq0\); \hfill Nonnegative
	\item \(||A||=0\implies A=0\); \hfill Positive
	\item \(||cA||=|c|\cdot||A||\); \hfill Homogeneous
	\item \(||A+B||\leq||A||+||B||\); \hfill Subadditive
	\item \(||AB||\leq||A||\cdot||B||\). \hfill Submultiplicative
\end{enumerate}
\cite{horn}
\end{definition}



\begin{theorem}
The set of functions on \(A\in\C^{m\times n}\) with \(a_{ij}\in A,\forall i\in[1,m]\cap\N,j\in[1,n]\cap\N\),
\begin{equation}\label{matrixnorm}
N_p(A)\coloneqq
\begin{cases} 
\left(\sum_{i=1}^m\sum_{j=1}^n |a_{ij}|^p\right)^{\frac{1}{p}} & p\in\N^*\\
\max_{\substack{1\leq i\leq m\\ 1\leq j\leq n}}|a_{ij}| & p=+\infty
\end{cases}
\end{equation}
define a set of norms.
\end{theorem}
\begin{proof}
We separate the cases where \(p=+\infty\) and \(p\in\N^*\).
\begin{itemize}
	\item For \(p=+\infty\):
	\begin{enumerate}%[Property 1]
		\item Nonnegative:
		\[N_\infty(A)=\max_{\substack{1\leq i\leq m\\ 1\leq j\leq n}}|a_{ij}|\ge0. \]
		\item Positive:
		\[N_\infty(A)=0\implies\max_{\substack{1\leq i\leq m\\ 1\leq j\leq n}}|a_{ij}|=0\implies \forall i\in[1,m]\cap\N,j\in[1,n]\cap\N,|a_{ij}|=0, \]
		which means all the entries of $A$ are 0 and thus \(A=0\).
		\item Homogeneous:
		\[ N_\infty(cA)=\max_{\substack{1\leq i\leq m\\ 1\leq j\leq n}}|c\cdot a_{ij}|=\max_{\substack{1\leq i\leq m\\ 1\leq j\leq n}}|c|\cdot|a_{ij}|=|c|\max_{\substack{1\leq i\leq m\\ 1\leq j\leq n}}|a_{ij}|=|c|\cdot N_\infty(A). \]
		\item Subadditive:
		\begin{align*}
		N_\infty(A+B)&=\max_{\substack{1\leq i\leq m\\ 1\leq j\leq n}}|a_{ij}+b_{ij}|\leq\max_{\substack{1\leq i\leq m\\ 1\leq j\leq n}}(|a_{ij}|+|b_{ij}|)\\
		&\leq\max_{\substack{1\leq i\leq m\\ 1\leq j\leq n}}|a_{ij}|+\max_{\substack{1\leq i\leq m\\ 1\leq j\leq n}}|b_{ij}|=N_\infty(A)+N_\infty(B).
		\end{align*}
	\end{enumerate}
	\item For \(p\in\N^*\), only property 1,2 and 3 of the definition will be proved.
	The property 4 will be proved in the following context by three famous theorems: Young's inequality \ref{young}, H\"{o}lder's inequality \ref{holder} and Minkowski's inequality \ref{minkowski}.
	\begin{enumerate}
		\item Nonnegative:
		\[ N_p(A)=\left(\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^p\right)^{\frac{1}{p}}\ge0. \]
		\item Positive:
		\[N_p(A)=0\implies\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^p=0\implies \forall i\in[1,m]\cap\N,j\in[1,n]\cap\N,|a_{ij}|=0, \]
		which means all the entries of $A$ are 0 and thus \(A=0\).
		\item Homogeneous:
		\[ N_p(cA)=\left(\sum_{i=1}^m\sum_{j=1}^n|c\cdot a_{ij}|^p\right)^{\frac{1}{p}}=\left(|c|^p\cdot\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^p\right)^{\frac{1}{p}}=|c|\cdot N_p(A). \]
		\item As for subadditivity, we need to prove
		\[ N_p(A+B)\leq N_p(A)+N_p(B), \]
		which can be explicitly expressed as
		\[ \left(\sum_{i=1}^m\sum_{j=1}^n|a_{ij}+b_{ij}|^p\right)^{\frac{1}{p}}\leq\left(\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^p\right)^{\frac{1}{p}}+\left(\sum_{i=1}^m\sum_{j=1}^n|b_{ij}|^p\right)^{\frac{1}{p}}. \]
		The dimension of the matrix does not matter here, so that we can compress it to one dimensional sequences,
		\begin{equation}\label{minkowskii}
		\left[\sum_{i=1}^k|a_i+b_i|^p\right]^\frac{1}{p}\leq\left(\sum_{i=1}^k |a_i|^p\right)^\frac{1}{p}+\left(\sum_{i=1}^k |b_i|^p\right)^\frac{1}{p},
		\end{equation}
		where \(k\in\N^*\).
		And we can also extend the domain to \(p\in(1,+\infty)\), without the constraint of being an integer.
		This is the discrete sum case of Minkowski's inequality , which will be proved in the following context, with the support of H\"{o}lder's inequality.
	\end{enumerate}
\end{itemize}
\end{proof}



\begin{theorem}[Young's inequality]\label{young}
Let \(p,q\in\R_*^+\) be strictly positive real numbers such that \(\frac{1}{p}+\frac{1}{q}=1\).
Then, for any \(a,b\in\R^+\),
\begin{equation}
ab\leq\frac{a^p}{p}+\frac{b^q}{q}.
\end{equation}
Equality occurs if and only if
\[ b=a^{p-1}. \]
\end{theorem}
\begin{proof}
We write the left hand side of the inequality as
\[ ab=\exp\left(\frac{1}{p}\ln a^p+\frac{1}{q}\ln b^q\right). \]
Since the exponential function is strictly positive, we apply Jensen's inequality,
\[ \exp\left(\frac{1}{p}\ln a^p+\frac{1}{q}\ln b^q\right)\leq\frac{1}{p}\exp\left(\ln a^p\right)+\frac{1}{q}\exp\left(\ln b^q\right). \]
Apply the substitution and simplification, we get
\[ ab\leq\frac{a^p}{p}+\frac{b^q}{q}. \]
\end{proof}


\begin{theorem}[H\"{o}lder's inequality]\label{holder}
Let \(p,q\in(1,+\infty)\) be strictly positive real numbers such that \(\frac{1}{p}+\frac{1}{q}=1\).
Two sequences of positive real numbers \(a_i,b_i\in\R^+,\forall i\in[1,n]\cap\N\) satisfies
\begin{equation}
\sum_{i=1}^n a_ib_i\leq\left(\sum_{i=1}^n a_i^p\right)^{\frac{1}{p}}\left(\sum_{i=1}^n b_i^q\right)^{\frac{1}{q}}.
\end{equation}
Equality occurs if and only if \((\forall i, a_i=0)\) or \((\forall i, b_i=0)\) or \((\exists c_1,c_2\in\R_*^+\forall i,c_1a_i^p=c_2b_i^q)\).
\end{theorem}
\begin{proof}
If \(\forall i, a_i=0\) or \(\forall i, b_i=0\), both sides of the inequality are zero, and equality occurs trivially.
So we consider the case where the right hand side of the inequality is greater than 0.
Then it is equivalent to prove
\[ \frac{\sum_{i=1}^n a_ib_i}{\left(\sum_{i=1}^n a_i^p\right)^{\frac{1}{p}}\left(\sum_{i=1}^n b_i^q\right)^{\frac{1}{q}}}\leq1. \]
Then, we define
\[ x_j\coloneqq\frac{a_j}{\left(\sum_{i=1}^n a_i^p\right)^{\frac{1}{p}}}, \quad y_j\coloneqq\frac{b_j}{\left(\sum_{i=1}^n b_i^q\right)^{\frac{1}{q}}}, \quad \forall j\in[1,n]\cap\N. \]
Then it is equivalent to prove
\[ \sum_{j=1}^n x_jy_j\leq1. \]
Apply Young's inequality, we have
\[ x_jy_j\leq\frac{x_j^p}{p}+\frac{y_j^q}{q}, \quad \forall j\in[1,n]\cap\N. \]
So their sum satisfies
\[ \sum_{j=1}^n x_jy_j\leq \sum_{j=1}^n\left(\frac{x_j^p}{p}+\frac{y_j^q}{q}\right)=\frac{1}{p}\sum_{j=1}^n x_j^p+\frac{1}{q}\sum_{j=1}^n y_j^q. \]
According to the definition,
\[ \sum_{j=1}^n x_j^p=\frac{\sum_{j=1}^n a_j^p}{\sum_{i=1}^n a_i^p}=1. \]
Similarly,
\[ \sum_{j=1}^n y_j^q=\frac{\sum_{j=1}^n b_j^q}{\sum_{i=1}^n b_i^q}=1. \]
Therefore,
\[ \frac{\sum_{i=1}^n a_ib_i}{\left(\sum_{i=1}^n a_i^p\right)^{\frac{1}{p}}\left(\sum_{i=1}^n b_i^q\right)^{\frac{1}{q}}}=\sum_{j=1}^n x_jy_j\leq\frac{1}{p}+\frac{1}{q}=1. \]
\cite{utx}
\end{proof}


\begin{theorem}[Minkowski's inequality]\label{minkowski}
Let \(p\in(1,+\infty)\) be strictly positive real number.
Two sequences of positive real numbers \(a_i,b_i\in\R^+,\forall i\in[1,n]\cap\N\) satisfies
\begin{equation}
\left[\sum_{i=1}^n (a_i+b_i)^p\right]^\frac{1}{p}\leq\left(\sum_{i=1}^n a_i^p\right)^\frac{1}{p}+\left(\sum_{i=1}^n b_i^p\right)^\frac{1}{p}.
\end{equation}
\end{theorem}
\begin{proof}
Define
\[ q\coloneqq\frac{p}{p-1} \]
such that \(\frac{1}{p}+\frac{1}{q}=1\) is satisfied.
It follows from the H\"{o}lder's inequality that
\begin{align*}
\sum_{i=1}^n(a_i+b_i)^p&=\sum_{i=1}^na_i(a_i+b_i)^{p-1}+\sum_{i=1}^nb_i(a_i+b_i)^{p-1}\\
&\leq \left(\sum_{i=1}^na_i^p\right)^\frac{1}{p}\left[\sum_{i=1}^n(a_i+b_i)^{p}\right]^\frac{1}{q}
+\left(\sum_{i=1}^nb_i^p\right)^\frac{1}{p}\left[\sum_{i=1}^n(a_i+b_i)^{p}\right]^\frac{1}{q}\\
&=\left[\left(\sum_{i=1}^na_i^p\right)^\frac{1}{p}+\left(\sum_{i=1}^nb_i^p\right)^\frac{1}{p}\right]\left[\sum_{i=1}^n(a_i+b_i)^{p}\right]^\frac{1}{q}
\end{align*}
If \(\sum_{i=1}^n (a_i+b_i)^p=0\), the equality occurs trivially.
So we consider the case where \(\sum_{i=1}^n (a_i+b_i)^p>0\), where we can divide both sides by \(\left[\sum_{i=1}^n (a_i+b_i)^p\right]^{1/q}\) without changing the direction of the inequality.
Notice that \(1-\frac{1}{q}=\frac{1}{p}\), we get
\[ \left[\sum_{i=1}^n(a_i+b_i)^p\right]^\frac{1}{p}\leq\left(\sum_{i=1}^n a_i^p\right)^\frac{1}{p}+\left(\sum_{i=1}^n b_i^p\right)^\frac{1}{p}. \]
\cite{utx}
\end{proof}


With Minkowski's inequality, it is easy to prove (\ref{minkowskii}), and thus equation (\ref{matrixnorm}) defines a family of matrix norm, since the triangle inequality required by the norm is a special case of Minkowski's inequality.

\begin{definition}[Induced matrix norm]
The \emph{matrix norm induced by \(N_i\)} with \(i\in\N^*\) for real matrices \(||\cdot||_i:\R^{m\times n}\to\R^+\) is defined to be
\begin{equation}
||A||_i\coloneqq\sup_{\substack{x\neq0\\ x\in\R^n}}\frac{N_i(Ax)}{N_i(x)}=\sup_{\substack{N_i(x)=1 \\ x\in\R^n}}N_i(Ax)
\end{equation}
\end{definition}

\begin{definition}[Condition number]
If a matrix \(A\in\R^{n\times n}\) is invertible, then we can define the \emph{condition number} of $A$ denoted by \(\kappa_i(A)\) as
\begin{equation}
\kappa_i(A)\coloneqq||A||_i||A^{-1}||_i.
\end{equation}
\end{definition}
The condition number indicates the invertibility of a matrix.
Matrices with small condition number are called well-conditioned, whereas
those with a large condition number ill-conditioned.
In particular, they evaluate the quality of the Cholesky decomposition on matrices.
\cite{tam39}




\section{Numerical Algorithms}

\subsection{Cholesky Decomposition}
By Theorem \ref{cholesky}, we can write algorithms to implement Cholesky decomposition.
\begin{algorithm}[h]
		\KwIn{Positive definite square matrix \(A\in\R^{n\times n}\)}
		\KwOut{Lower triangular matrix $L$ such that \(LL^T=A\)}
		\For{k=1:n-1}
		{
			\(A(k,k)=\sqrt{A(k,k)}\)\;
			\For{j=k+1:n}
			{
				\(A(j,k)=A(j,k)/A(k,k)\)\;
			}
			\For{j=k+1:n}
			{
				\For{m=j:n}
				{
					\(A(m,j)=A(m,j)-A(m,k)\cdot A(j,k)\)\;
				}
			}
		}
		\(A(n,n)=\sqrt{A(n,n)}\)\;
		\(L=(a_{ij})_{j\leq i}\)\;
		\caption{Cholesky Decomposition\cite{tam37}}
\end{algorithm}

\subsection{Complexity Analysis}
The Cholesky decomposition algorithm has time complexity \(O\left(n^3\right)\), since there are three contained loop, each of which are of complexity \(O(n)\).


\newpage
\appendix
\section{Appendix}
\subsection{MATLAB\texttrademark\ Code for Cholesky Decomposition}
\lstinputlisting[style=Matlab-editor]{Cholesky.m}