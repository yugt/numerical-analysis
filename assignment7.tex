%!TEX root = main.tex
\titleformat{\section}{\normalfont\large\bfseries}{Exercise \thesection\ --- }{1pt}{}
% \setcounter{chapter}{6}
\renewcommand{\chaptername}{Assignment}
\chapter{Iteration Methods}


\section{Lipschitz continuity vs. Differentiability}
A function \(f:\R\to\R\) is differentiable in some open interval \(\mathcal{I}\subset\R\) if it is differentiable at every point of \(\mathcal{I}\), and it is Lipschitz continuous if there is a constant \(c\geq0\) such that
\[ |f(x_1)-f(x_2)\leq c|x_1-x_2| \qquad \forall x_1,x_2\in\mathcal{I}. \]
\begin{enumerate}
	\item Give a function, with proof, that is differentiable but its derivative is not bounded in some open interval.
	\begin{proof}
	\[ f(x)=\tan x, \quad x\in\left(-\frac{\pi}{2},\frac{\pi}{2}\right) \]
	is differentiable since its derivative is \(f'(x)=\sec^2x\), which is not bounded on its domain.
	\end{proof}
	\item Suppose $f$ is differentiable and $f'$ is bounded in some open interval.
	Prove that $f$ is Lipschitz continuous in that interval.
	\begin{proof}
	We first introduce Cauchy's Mean Value Theorem,
	\begin{theorem}[Mean Value Theorem]
	\label{cauchymean}
	If a function $f$ is continuous on the closed interval \([a,b]\), and differentiable on the open interval \((a,b)\), then there exists a point \(\xi\in(a,b)\) such that
	\[ f'(\xi)=\frac{f(b)-f(a)}{b-a}. \]
	\end{theorem}
	\begin{proof}
	Define \(g(x)=f(x)-rx\), where $r$ is a constant.
	Since $f$ is continuous on \([a,b]\) and {}differentiable on \((a,b)\), the same is true for $g$.
	We now want to choose $r$
	\begin{align*}
	g(a)=g(b)&\iff f(a)-ra=f(b)-rb\\
	&\iff r(b-a)=f(b)-f(a) \\
	&\iff r=\frac{f(b)-f(a)}{b-a}.
	\end{align*}
	By Rolle's theorem \ref{rolle}, since $g$ is differentiable and \(g(a)=g(b)\), there is some \(\xi\in(a,b)\) for which \(g'(\xi)=0\) and thus 
	\[ f'(\xi)=g'(\xi)+r=\frac{f(b)-f(a)}{b-a}\]
	as required.
	\end{proof}
	Then, we can set the Lipschitz constant to be
	\[ c=\sup_{\xi\in(a,b)}|f'(\xi)|. \]
	Since $f'$ is bounded on \((a,b)\), the Lipschitz constant is finite.
	Apply Cauchy's Mean Value Theorem, we get
	\[ \forall x_1,x_2\in(a,b),\quad |f(x_1)-f(x_2)|=|f'(\xi)|\cdot|x_1-x_2|\leq c\cdot|x_1-x_2|, \]
	which means $f$ is Lipschitz continuous.
	\end{proof}
	\item Give a function, with proof, that is differentiable but not Lipschitz continuous in some open interval.
	\begin{proof}
	This is exactly the same as differentiable but the derivative not bounded.	
	\[ f(x)=\sqrt[3]{x}, \quad x\in(-1,1) \]
	is differentiable since its derivative is \(f'(x)=\frac{1}{3}x^{-\frac{2}{3}}\).
	But it is not Lipschitz continuous since
	\[ \frac{|f(x_1)-f(x_2)|}{|x_1-x_2|}=\frac{1}{x_1^{\frac{2}{3}}+x_1^{\frac{1}{3}}x_2^{\frac{1}{3}}+x_2^{\frac{2}{3}}} \]
	is not bounded on \(x_1, x_2\in(-1,1)\) so that the Lipschitz constant does not exist.
	\end{proof}
	\item Give a function, with proof, that is Lipschitz continuous but not differentiable in some open interval.
	\begin{proof}
	\[ f(x)=|x|, \quad x\in(-1,1) \]
	is Lipschitz continuous with Lipschitz constant
	\[ c=\limsup_{x_1,x_2\in(-1,1)}\left|\frac{f(x_1)-f(x_2)}{x_1-x_2}\right|=1. \]
	But it is not differentiable at \(x=0\).
	\end{proof}
\end{enumerate}


\section{Fixed point iteration convergence condition\label{fixedpoint}}
A fixed point of a function \(g(x)\) is a real number \(x^*\) such that
\[ g(x^*)=x^*. \]
Assume the followings:
\begin{enumerate}
	\item The function $g$ and its derivative $g'$ are continuous on \([a, b]\), \textit{i.e.}, \(g,g'\in\mathcal{C}[a,b]\).
	\item The function $g$ is bounded below by $a$ and above by $b$, \textit{i.e.}, \(\forall x\in[a,b],g(x)\in[a, b]\).
	\item The initial point \(x_0\) is an interior point of \([a,b]\), \textit{i.e.}, \(x_0\in(a, b)\).
\end{enumerate}
Show the followings are true.
\begin{enumerate}
	\item If \(0\leq|g'(x)|<1, \forall x\in[a,b]\), the fixed-point iteration will converge to the unique fixed point \(x^*\in[a,b]\).
	\begin{proof}
	Define the Lipschitz constant to be
	\[ c=\sup_{\xi\in(a,b)} g'(\xi). \]
	It comes from definition that \(0\leq c<1\).
	Define \(h(x)=g(x)-x\), and it is monotonically decreasing since
	\[ h'(x)=g'(x)-1\in(-2,0), \quad \forall x\in[a,b]. \]
	Thus, \(h(x)=0\) has at most one solution, and so is \(g(x)=x\).
	This proves the uniqueness of \(x^*\).
	Then apply Cauchy's Mean Value Theorem \ref{cauchymean},
	\[ |g(x_{k+1})-g(x_k)|=|g'(\xi)|\cdot|x_{k+1}-x_k|\leq c\cdot|x_{k+1}-x_k|. \]
	Apply the iteration relation,
	\[ x_{k+1}=g(x_k), \qquad x_k=g(x_{x-1}), \]
	we get
	\[ |g(x_{k+1})-g(x_k)|\leq c\cdot|g(x_k)-g(x_{k-1})|. \]
	Such a recursive relation deduces
	\[ |g(x_{k+1})-g(x_k)|\leq c^k\cdot|g(x_1)-g(x_0)|. \]
	Notice that \(g(x)\) is bounded on \([a,b]\), so \(|g(x_1)-g(x_0)|\) is finite, and with \(0\leq c<1\),
	\[ \lim_{k\to\infty}|g(x_{k+1})-g(x_k)|\leq |g(x_1)-g(x_0)|\cdot\lim_{k\to\infty}c^k=0. \]
	Furthermore, the iteration relation also implies
	\[ \lim_{k\to\infty}(x_{k+2}-x_{k+1})=0. \]
	Together with the boundedness of \(x_n\) and \(g\), we conclude that the sequence \(x_n\) is convergent to some \(x^*\in[a,b]\).
	\end{proof}
	\item If \(|g'(x)|>1, \forall x\in[a,b]\), then the fixed-point iteration will never converge to \(x^*\).
	\begin{proof}
	Since \(g'(x)\) is continuous on the interval \([a,b]\),
	\[ |g'(x)|>1 \implies (\forall x\in[a,b], g'(x)>1) \quad \text{or} \quad (\forall x\in[a,b], g'(x)<-1) \]
	So we discuss over these two cases.
	\begin{itemize}
		\item{\(g'(x)>1\):} define \(h(x)=g(x)-x\), and it is monotonically increasing since
		\[ h'(x)=g'(x)-1>0. \]
		Thus, \(h'(x)=0\) has at most one solution, and so is \(g(x)=x\).
		\item{\(g'(x)<-1\):} define \(h(x)=g(x)+x\), and it is monotonically decreasing since
		\[ h'(x)=g'(x)+1<0. \]
		Thus, \(h'(x)=0\) has at most one solution, and so is \(g(x)=x\).
	\end{itemize}
	In both cases, if the solution \(x^*\) exists, then it is unique.
	Define the Lipschitz constant to be
	\[ c=\inf_{\xi\in(a,b)} |g'(\xi)|. \]
	It comes from definition that \(c>1\).
	Then apply Cauchy's Mean Value Theorem \ref{cauchymean},
	\[ |g(x_{k+1})-g(x_k)|=|g'(\xi)|\cdot|x_{k+1}-x_k|\geq c\cdot|x_{k+1}-x_k|. \]
	Apply the iteration relation,
	\[ x_{k+1}=g(x_k), \qquad x_k=g(x_{x-1}), \]
	we get
	\[ |g(x_{k+1})-g(x_k)|\geq c\cdot|g(x_k)-g(x_{k-1})|. \]
	Such a recursive relation deduces
	\[ |g(x_{k+1})-g(x_k)|\geq c^k\cdot|g(x_1)-g(x_0)|=c^{k+1}|x_1-x_0|. \]
	Since \(x^*\) is unique, if \(x_0\neq x^*\), then \(x_1=g(x_0)\neq x_0\).
	Then, with \(c>1\),
	\[ \lim_{k\to\infty}|g(x_{k+1})-g(x_k)|\geq |x_1-x_0|\lim_{k\to\infty} c^{k+1}=+\infty \]
	which implies that the iteration does not converge.
	\end{proof}
\end{enumerate}


\section{Root finding}
Choose a suitable numerical method to find the smallest positive root and the second smallest positive root of the equation
\[ \tan x = 4x \]
correct to 3 decimal places.
Explain your choice.
\begin{proof}[Answer]
Newton's method converges quadratically if the function has nonzero derivative at the root, which is faster than bisection method.
Even if it has zero derivative at the root, Newton's method still converges linearly, which is not worse than bisection method.
So we choose Newton's method.
We first define the function in MATLAB\texttrademark\ style.
\begin{lstlisting}[style=Matlab-editor]
f = @(x) tan(x)-4*x;
df = @(x) 1/(cos(x)^2)-4;
\end{lstlisting}
The initial guess for the first positive root is near \(\frac{\pi}{2}\), while the second is near \(\frac{3\pi}{2}\).
Using the MATLAB\texttrademark\ function provided, we perform the iteration with the following command:
\begin{lstlisting}[style=Matlab-editor]
newton(f,df,1.57,1e-4,1e-3,100)
newton(f,df,4.7,1e-4,1e-3,100)
\end{lstlisting}
The results are recorded in Table \ref{newtoniteration}.
\ifnum\webview=1
	\begin{table}[H]
	\centering
	\begin{subtable}[t]{.8\textwidth}
\else
	\begin{table}[htbp]
	\begin{subtable}[t]{.48\linewidth}
\fi
		\centering
		\caption{Finding the first positive root}
		\begin{tabular}[t]{|c|c|c|c|}
		\hline
		$n$ & \(x_n\) & \(E_n\) & Relative \(E_n\) \\	\hline
		1	&	1.5692	&	7.9235e-04	&	0.0001\\	\hline
		2	&	1.5676	&	0.0016	&	0.0020	\\	\hline
		3	&	1.5645	&	0.0031	&	0.0040	\\	\hline
		4	&	1.5585	&	0.0060	&	0.0077	\\	\hline
		5	&	1.5472	&	0.0113	&	0.0147	\\	\hline
		6	&	1.5270	&	0.0202	&	0.0265	\\	\hline
		7	&	1.4947	&	0.0323	&	0.0432	\\	\hline
		8	&	1.4525	&	0.0422	&	0.0581	\\	\hline
		9	&	1.4141	&	0.0384	&	0.0543	\\	\hline
		10	&	1.3959	&	0.0181	&	0.0260	\\	\hline
		11	&	1.3933	&	0.0026	&	0.0038	\\	\hline
		12	&	1.3932	&	4.5560e-05	&	6.5397e-05	\\	\hline
		\end{tabular}
	\end{subtable}
	\ifnum\webview=1
		\\
		\centering
		\begin{subtable}[t]{.8\textwidth}
	\else
		\hfill
		\begin{subtable}[t]{.48\linewidth}
	\fi
		\centering
		\caption{Finding the second positive root}
		\begin{tabular}[t]{|c|c|c|c|}
		\hline
		$n$ & \(x_n\) & \(E_n\) & Relative \(E_n\) \\	\hline
		1	&	4.6905	&	0.0095	&	0.0041	\\	\hline
		2	&	4.6776	&	0.0129	&	0.0055	\\	\hline
		3	&	4.6654	&	0.0122	&	0.0052	\\	\hline
		4	&	4.6596	&	0.0058	&	0.0025	\\	\hline
		5	&	4.6588	&	8.1071e-04	&	3.4803e-04	\\	\hline
		6	&	4.6588	&	1.2777e-05	&	5.4849e-06	\\	\hline
		\end{tabular}
	\end{subtable}
	\caption{Newton's Method on Iteration}
	\label{newtoniteration}
\end{table}

\end{proof}



\section{Order and rate of convergence}
Suppose that the sequence \(\{a_n\}\) converges to the number $L$, and there is a constant \(0<\lambda<\infty\) such that
\[ \lim_{n\to\infty} \frac{|a_{n+1}-L|}{|a_n-L|^\alpha}=\lambda, \]
then the sequence is said to converge to $L$ with \emph{order of convergence} $\alpha$.
The constant $\lambda$ is called the \emph{asymptotic error}.
A positive sequence \(\{E_n\}\) is said to has an order of at least $\alpha$ and a \emph{rate} of at most $\lambda$ if there is a sequence \(\{a_n\}\) that has an order $\alpha$ and a rate of $\lambda$ such that
\[ E_n\leq a_n \quad \forall n\in\N^*. \]
\begin{enumerate}
	\item Find the order of convergence and the rate of convergence of the sequence
	\[ a_n=\frac{1}{n} \quad \forall n\geq1 \]
	\begin{proof}[Answer]
	First, we evaluate the sequence limit.
	\[ L=\lim_{n\to\infty}\frac{1}{n}=0. \]
	Then,
	\[ \lim_{n\to\infty}\frac{|a_{n+1}|}{|a_n|}=1 \]
	So the order of the convergence is 1, and the rate of the convergence is also 1.
	\end{proof}
	\item Examine the order of convergence and the rate of convergence of \(\{b_n\}\) where
	\[ b_{2k}=\frac{1}{\ln k} \quad \text{and} \quad b_{2k+1}=\frac{1}{k} \quad \forall k\geq1. \]
	\begin{proof}[Answer]
	First, we evaluate the sequence limit.
	\[ L=\lim_{n\to\infty}\max\left(\frac{1}{\ln (n/2)},\frac{2}{n}\right)=0. \]
	Notice that
	\[ \forall \alpha\in\R,\qquad \lim_{n\to\infty}\frac{\ln^\alpha n}{n}=0 \quad\text{and}\quad \lim_{n\to\infty}\frac{n}{\ln^\alpha n}=+\infty. \]
	Then,
	\begin{align*}
		\lim_{2k\to\infty}\frac{|b_{2k+1}|}{|b_{2k}|}&=\lim_{k\to\infty}\frac{\ln k}{k}=0\\
		\lim_{2k\to\infty}\frac{|b_{2k+2}|}{|b_{2k+1}|}&=\lim_{k\to\infty}\frac{k+1}{\ln k}=+\infty
	\end{align*}
	Since the two subsequence of \(b_n\) have two different limits, the limit of \(b_n\) does not exist.
	So neither the order of the convergence nor the rate of the convergence exists.
	\end{proof}
	\item Use the precise definition above to show the method of fixed-point iteration leads an error sequence that has at least linear convergence.
	\begin{proof}
	Suppose the fixed point for function $g$ is
	\[ g(x^*)=x^*. \]
	The iteration gives
	\[ \forall n\in\N^*,\quad x_{n+1}=g(x_n). \]
	If the iteration converges, according to \ref{fixedpoint}, we have
	\[ \lim_{n\to\infty} x_n=x^*. \]
	The linear convergence comes directly from the definition of Lipschitz continuity.
	Lipschitz continuity ensures that
	\[ |g(x_{n})-g(x^*)|\leq c\cdot|x_n-x^*|, \]
	where \(0<c<1\) is the Lipschitz constant.
	Apply substitutions, we obtain
	\[ |x_{n+1}-x^*|\leq c\cdot|x_n-x^*| \]
	Apply the definition of error term,
	\[ E_{n+1}\leq c\cdot E_n. \]
	Since this is true for all \(n\in\N^*\), we have the limit
	\[ \lim_{n\to\infty} \frac{E_{n+1}}{E_n}\leq c. \]
	So the linear convergence is proved.
	\end{proof}
	\item Find the asymptotic error constant for the method of fixed-point iteration when it has at least quadratic convergence.
	\begin{proof}
	Since \(g:\R\to\R\) is required to be Lipschitz continuous, it is absolutely continuous, and therefore is almost everywhere differentiable.
	Apply the Taylor's expansion at \(x^*\),
	\[ g(x_n)=g(x^*+(x_n-x^*))=g(x^*)+g'(x^*)(x_n-x^*)+\frac{1}{2}g''(x^*)(x_n-x^*)^2+o\left((x_n-x^*)^2\right) \]
	If the convergence is at least quadratic, the first derivative \(g'(x^*)=0\), so the Taylor expansion is reduced to
	\[ x_{n+1}-x^*=\frac{1}{2}g''(x^*)(x_n-x^*)^2+o\left((x_n-x^*)^2\right) \]
	Observe that \(x_n-x^*\) is the error term \(E_n\), and by substitution,
	\[ \lim_{n\to\infty}\frac{E_{n+1}}{E_n} = g'(x^*). \]
	So the convergence rate is
	\[ \lim_{n\to\infty}\frac{|x_{n+1}-x^*|}{|x_{n}-x^*|^2}=\frac{1}{2}|g'(x^*)|. \]
	\end{proof}
	\item Show the order of convergence of the secant method is \(\frac{\sqrt{5}+1}{2}\).
	\begin{proof}
		Apply the definition of secant method.
		\[ x_{n+1}=x_n-\frac{f(x_n)(x_n-x_{n-1})}{f(x_n)-f(x_{n-1})}. \]
		Rewrite it in the form of error term, \textit{i.e.}, apply the substitution \(x_n=x_*+E_n\), we get
		\begin{equation}
		\label{secanterror}
		E_{n+1}=E_n-\frac{f(x^*+E_n)(E_n-E_{n-1})}{f(x^*+E_n)-f(x^*+E_{n-1})}.
		\end{equation}
		Assume \(f\in\mathcal{C}^2\), we apply Taylor's expansion up to the quadratic term,
		\[ f(x^*+E)\approx f(x^*)+f'(x^*)E+\frac{f''(x^*)}{2}\cdot E^2. \]
		The subscript is ignored since Taylor's expansion applies to all \(E_n,n\in\N^*\) with the assumption that \(E_n\) are small.
		Since \(f(x^*)=0\) by definition of root,
		\[ f(x^*+E_n)\approx f'(x^*)E_n \left(1+\frac{f''(x^*)}{2f'(x^*)}\cdot E_n \right). \]
		So the denominator in (\ref{secanterror}) can be approximated by
		\[ f(x^*+E_n)-f(x^*+E_{n-1})\approx f'(x^*)(E_n-E_{n-1}) \left[1+\frac{f''(x^*)}{2f'(x^*)}\cdot (E_n+E_{n-1}) \right]. \]
		For simplicity, define
		\[ M=\frac{f''(x^*)}{2f'(x^*)}. \]
		Then equation (\ref{secanterror}) can be simplified to
		\begin{align*}
			E_{n+1}&\approx E_n-\frac{E_n f'(x^*)(1+M E_n)(E_n-E_{n-1})}{f'(x^*)(E_n-E_{n-1})[1+M(E_n+E_{n-1})]}\\
			&=E_n-\frac{E_n (1+M E_n)}{1+M(E_n+E_{n-1})}\\
			&=\frac{M}{1+M(E_n+E_{n-1})}E_nE_{n-1}
		\end{align*}
		Again, apply the assumption that \(E_n\) are small, then
		\[ \frac{1}{1+M(E_n+E_{n-1})}\approx 1, \]
		and thus
		\begin{equation}
		\label{secantrecursion}
		E_{n+1}=M E_n E_{n-1}.
		\end{equation}
		By definition of convergence rate,
		\[ \lim_{n\to\infty}\frac{|E_{n+1}|}{|E_n|^\alpha}=\lambda. \]
		Then, we have the approximated first order recursive relation
		\[ \forall n>N,\quad|E_{n+1}|\approx \lambda|E_n|^\alpha. \]
		Apply this approximation to the second order recursive relation (\ref{secantrecursion}),
		\[ \lambda|E_n|^\alpha=|M|\cdot|E_n|\cdot|E_{n-1}|, \]
		which can be written in another first order recursive relation
		\[ |E_n|^{\alpha-1}=\frac{|M|}{\lambda}\cdot|E_{n-1}|. \]
		Taking the \((\alpha-1)^{\text{th}}\) root, we have the recursive relation in the original form
		\[ |E_n|=\left(\frac{|M|}{\lambda}\right)^{\frac{1}{\alpha-1}}\cdot|E_{n-1}|^{\frac{1}{\alpha-1}} \]
		Since we only focus on the order of convergence, not the rate, we omit the equation for the coefficient.
		Then, we require
		\[  \alpha=\frac{1}{\alpha-1}, \]
		which has a root
		\[ \alpha=\frac{\sqrt{5}+1}{2}. \]
		The negative root is omitted since it does not satisfy the convergence condition.
	\end{proof}
\end{enumerate}


\section{Order of roots}
Suppose that \(f(x)\) and its derivatives \(f'(x),\cdots, f^{(m)}(x)\) are defined and continuous on an interval containing $x^*$, we say that \(f(x)=0\) has a \emph{root of order} $m$ at \(x=x^*\) if and only if
\[ f(x^*)=0, f'(x^*)=0, f''(x^*)=0, \cdots, f^{(m-1)}(x^*)=0, \text{ and } f^{(m)}(x^*)\neq0. \]
The positive integer $m$ is known as the \emph{multiplicity of the root}.
A root of order \(m=1\) is often called a \emph{simple root}, and if \(m>1\), it is called a \emph{multiple root}.
\begin{enumerate}
	\item Prove by induction that there exists a continuous function \(h(x)\) so that \(f(x)\) can be expressed as the product
	\[ f(x)=(x-x^*)^m h(x), \quad \text{where} \quad h(x^*)\neq0 \]
	if the equation \(f(x)\) has a root of order $m$ at \(x=x^*\).
	\begin{proof}
		Actually, induction is not required to prove the statement.
		We can define
		\[ h(x)=\frac{f(x)}{(x-x^*)^m}, \quad \forall x\in\R\backslash\{x^*\}. \]
		This function is obviously continuous at \(\R\backslash\{x^*\}\).
		So we need the continuity at \(x=x^*\).
		Apply the L'Hospital rule, \(h\) has a limit at \(x=x^*\)
		\[ \lim_{x\to x^*}\frac{f(x)}{(x-x^*)^m}=\lim_{x\to x^*}\frac{f^{(m)}(x)}{\left[(x-x^*)^m\right]^{(m)}}		=\frac{f^{(m)}(x)}{m!}\neq0 \]
		by definition.
		So we complete the definition of \(h\):
		\[ h(x)=\begin{cases}
		\frac{f(x)}{(x-x^*)^m}, & x\in\R\backslash\{x^*\}\\
		\frac{f^{(m)}(x^*)}{m!}, & x=x^* \end{cases}. \]
		This is continuous at \(x^*\), since the limit of the function equals to the function value.
		It is easy to verify
		\[ f(x)=(x-x^*)^m h(x). \]
		And \(h(x^*)=f^{(m)}(x^*)\neq0\) is also satisfied.
		So we are able to find such an \(h(x)\) for all \(m\in\N^*\).
	\end{proof}
	\item Show the following modified Newton's iteration will produce a sequence that converges quadratically to $x^*$
	\[ x_k=x_{k-1}-m\cdot\frac{f(x_{k-1})}{f'(x_{k-1})} \]
	if the original Newton's method produces a sequence converges linearly to the root \(x=x^*\) of order \(m>1\).
	\begin{proof}
		Write \(f(x)=(x-x^*)^m h(x)\), then
		\[ x_{k} = x_{k-1} - m\cdot\frac{f(x_{k-1})}{f'(x_{k-1})} = x_{k-1} - 
		\frac{(x_{k-1}-x^*)h(x_{k-1})}{h(x_{k-1})+\frac{1}{m}(x_{k-1}-x^*)h'(x_{k-1})}. \]
		Define the error term \(E_k=x_k-x^*\), then
		\[ E_k = E_{k-1} \left( 1 - 
		\frac{1}{1+\frac{1}{m} E_{k-1}\frac{h'(x^*+E_{k-1})}{h(x^*+E_{k-1})}}\right). \]
		A first order approximation gives
		\[ E_k \approx E_{k-1}^2 \cdot
		\frac{h'(x^*+E_{k-1})}{m\cdot h(x^*+E_{k-1})}. \]
		So the quadratic convergence is proved.
	\end{proof}
	\item In practice, it is unlikely that $m$ is known, in those cases the following version of modified Newton's method can be applied to accelerate convergence
	\[ x_k=x_{k-1}-\frac{u(x_{k-1})}{u'(x_{k-1})}, \quad \text{where} \quad u(x)=\frac{f(x)}{f'(x)} \]
	Explain how this helps to speed up convergence.
	\begin{proof}[Answer]
		 If a polynomial \(f(x)\) has a root of multiplicity \(m\) at a point \(x_0\), then its derivative \(f'(x)\) has a root of multiplicity \(m-1\) at \(x_0\).
		 Thus we can obtain a function
		 \[ u(x)=\frac{f(x)}{f'(x)} \]
		 that  has  all  simple roots at all of \(f\)'s roots.
	\end{proof}
	\item Zero is a root of multiplicity of 3 for the function
	\[ f(x)=\sin(x^3). \]
	Start with \(x_0=1\) and compute \(x_1, x_2, \cdots, x_6\) first by using the iteration formula in part (b) and then by using the iteration formula in part (c).
	What can you conclude based on the values that you have computed?
	\begin{proof}[Answer]
	Using the MATLAB\texttrademark\ function provided, we perform the iteration with the following command:
	\begin{lstlisting}[style=Matlab-editor]
	f = @(x) sin(x^3);
	df = @(x) 3*x^2*cos(x^3);
	newton_Modified(f,df,1,3,1e-10,1e-10,100)
	u = @(x) tan(x^3)/(3*x^2);
	du = @(x) sec(x^3)^2 - (2*tan(x^3))/(3*x^3);
	newton_Improved(f,u,du,1,1e-20,1e-20,6)
	\end{lstlisting}
	The results are recorded in Table \ref{newtoncompare1}.
	\ifnum\webview=1
		\begin{table}[H]
		\centering
		\begin{subtable}[t]{.8\textwidth}
	\else
		\begin{table}[htbp]
		\begin{subtable}[t]{.49\linewidth}	
	\fi		
			\centering
			\caption{Modified with \(m=3\)}
			\begin{tabular}[t]{|c|c|c|}
			\hline
			\normalsize	$n$	&	\normalsize	\(x_n\)	&	\normalsize	\(E_n=|x_n-x_{n-1}|\)	\\	\hline
			\scriptsize	1	&	\scriptsize	-0.557407724654902		&	\scriptsize	1.557407724654902	\\	\hline
			\scriptsize	2	&	\scriptsize	0.005640692477577		&	\scriptsize	0.563048417132479	\\	\hline
			\scriptsize	3	&	\scriptsize	-6.071532165918825e-17	&	\scriptsize	0.005640692477577	\\	\hline
			\scriptsize	4	&	\scriptsize	0						&	\scriptsize	6.071532165918825e-17	\\	\hline
			\scriptsize	5	&	\scriptsize	NaN						&	\scriptsize	NaN	\\	\hline
			\scriptsize	6	&	\scriptsize	NaN						&	\scriptsize	NaN	\\	\hline
			\end{tabular}
		\end{subtable}
		\ifnum\webview=1
			\centering
			\begin{subtable}[t]{.8\textwidth}
		\else
			\begin{subtable}[t]{.49\linewidth}
		\fi
			\centering
			\caption{Improved}
			\begin{tabular}{|c|c|}
			\hline
			\normalsize	\(x_n\)	&	\normalsize	\(E_n=|x_n-x_{n-1}|\)	\\	\hline
			\scriptsize	0.782537832379215		&	\scriptsize	0.217462167620785	\\	\hline
			\scriptsize	0.265581322231385		&	\scriptsize	0.516956510147830	\\	\hline
			\scriptsize	1.862855151222864e-04	&	\scriptsize	0.265395036716262	\\	\hline
			\scriptsize	2.710505431213761e-20	&	\scriptsize	1.862855151222864e-04	\\	\hline
			\scriptsize	3.009265538105056e-36	&	\scriptsize	2.710505431213761e-20	\\	\hline
			\scriptsize	3.340955887615245e-52	&	\scriptsize	3.009265538105056e-36	\\	\hline
			\end{tabular}
		\end{subtable}
		\caption{Comparison of Newton's Method on Iteration}
		\label{newtoncompare1}
	\end{table}
	It's easy to observe that both methods converge rapidly, and the method in part (b) happens to reach the exact solution and terminates.
	\end{proof}
	\item Compare Newton's method and the modified version of it in part (c) by finding all the roots of the following polynomial correct to 6 decimal places.
	\[ f(x)=x^5 - 11x^4 + 46x^3 - 90x^2 + 81x - 27 \]
	\begin{proof}
	Using the MATLAB\texttrademark\ function provided, we perform the iteration with the following command:
	\begin{lstlisting}[style=Matlab-editor]
	f = @(x) x^5-11*x^4+46*x^3-90*x^2+81*x-27;
	df = @(x) 5*x^4-44*x^3+138*x^2-180*x+81;
	u = @(x) f(x)/df(x);
	du = @(x) (5*x^2-18*x+21)/(5*x-9)^2;
	newton_Modified(f,df,1.2,1,1e-11,1e-11,100)
	newton_Improved(f,u,du,1.2,1e-20,1e-20,6)
	newton_Modified(f,df,3.01,1,1e-20,1e-80,100)
	newton_Improved(f,u,du,3.02,1e-80,1e-40,40)
	\end{lstlisting}
	The results are recorded in Table \ref{newtoncompare2} and \ref{newtoncompare3}.
	\ifnum\webview=1
		\begin{table}[H]
		\centering
		\begin{subtable}[t]{.8\textwidth}
	\else
		\begin{table}[htbp]
		\begin{subtable}[t]{.49\linewidth}
	\fi	
			\centering
			\caption{Original Newton's iteration}
			\begin{tabular}[t]{|c|c|c|}
			\hline
			$n$	&	\(x_n\)				&	\(E_n=|x_n-x_{n-1}|\)	\\	\hline
			\footnotesize	1	&	\footnotesize	1.079999999999995	&	\footnotesize	0.120000000000005	\\	\hline
			\footnotesize	2	&	\footnotesize	1.037333333333312	&	\footnotesize	0.042666666666683	\\	\hline
			\footnotesize	3	&	\footnotesize	1.018118414918420	&	\footnotesize	0.019214918414892	\\	\hline
			\footnotesize	4	&	\footnotesize	1.008933250933831	&	\footnotesize	0.009185163984589	\\	\hline
			\footnotesize	5	&	\footnotesize	1.004436361406744	&	\footnotesize	0.004496889527087	\\	\hline
			\footnotesize	6	&	\footnotesize	1.002210759058684	&	\footnotesize	0.002225602348059	\\	\hline
			\footnotesize	7	&	\footnotesize	1.001103541654488	&	\footnotesize	0.001107217404196	\\	\hline
			\footnotesize	8	&	\footnotesize	1.000551313519913	&	\footnotesize	5.522281345746727e-04	\\	\hline
			\footnotesize	9	&	\footnotesize	1.000275542702326	&	\footnotesize	2.757708175875617e-04	\\	\hline
			\footnotesize	10	&	\footnotesize	1.000137742869052	&	\footnotesize	1.377998332738883e-04	\\	\hline
			\footnotesize	11	&	\footnotesize	1.000068864318036	&	\footnotesize	6.887855101522789e-05	\\	\hline
			\footnotesize	12	&	\footnotesize	1.000034430381744	&	\footnotesize	3.443393629209979e-05	\\	\hline
			\footnotesize	13	&	\footnotesize	1.000017214700528	&	\footnotesize	1.721568121615391e-05	\\	\hline
			\footnotesize	14	&	\footnotesize	1.000008607270650	&	\footnotesize	8.607429878404460e-06	\\	\hline
			\footnotesize	15	&	\footnotesize	1.000004303573453	&	\footnotesize	4.303697196395007e-06	\\	\hline
			\footnotesize	16	&	\footnotesize	1.000002151611564	&	\footnotesize	2.151961889129694e-06	\\	\hline
			\footnotesize	17	&	\footnotesize	1.000001075651434	&	\footnotesize	1.075960130103582e-06	\\	\hline
			\end{tabular}
		\end{subtable}
		\ifnum\webview=1
			\centering
			\begin{subtable}[t]{.8\textwidth}
		\else
			\begin{subtable}[t]{.49\linewidth}
		\fi
			\centering
			\caption{Improved}
			\begin{tabular}{|c|c|}
				\hline
				\(x_n\)				&	\(E_n=|x_n-x_{n-1}|\)	\\	\hline
				\footnotesize	1.036363636363630	&	\footnotesize	0.163636363636370	\\	\hline
				\footnotesize	1.001028277634942	&	\footnotesize	0.035335358728688	\\	\hline
				\footnotesize	1.000000793830820	&	\footnotesize	0.001027483804122	\\	\hline
				\footnotesize	1.000000000564978	&	\footnotesize	7.932658419029792e-07	\\	\hline
			\end{tabular}
		\end{subtable}
		\caption{Comparison of Newton's Method on Iteration at a Root of Order 2}
		\label{newtoncompare2}
	\end{table}
	\ifnum\webview=1
		\begin{table}[H]
		\centering
		\begin{subtable}[t]{.8\textwidth}
	\else
		\begin{table}[htbp]
		\begin{subtable}[t]{.49\linewidth}
	\fi	
			\centering
			\caption{Original Newton's iteration}
			\begin{tabular}{|c|c|c|}
			\hline
			$n$	&	\(x_n\)				&	\(E_n=|x_n-x_{n-1}|\)	\\	\hline
			\footnotesize	1	&	\footnotesize	3.006677685778723	&	\footnotesize	0.003322314221277	\\	\hline
			\footnotesize	2	&	\footnotesize	3.004456717701055	&	\footnotesize	0.002220968077668	\\	\hline
			\footnotesize	3	&	\footnotesize	3.002973344359819	&	\footnotesize	0.001483373341236	\\	\hline
			\footnotesize	4	&	\footnotesize	3.001983210661162	&	\footnotesize	9.901336986564147e-04	\\	\hline
			\footnotesize	5	&	\footnotesize	3.001322573018363	&	\footnotesize	6.606376427993332e-04	\\	\hline
			\footnotesize	6	&	\footnotesize	3.000881914953126	&	\footnotesize	4.406580652367431e-04	\\	\hline
			\footnotesize	7	&	\footnotesize	3.000588039669541	&	\footnotesize	2.938752835852654e-04	\\	\hline
			\footnotesize	8	&	\footnotesize	3.000392066202243	&	\footnotesize	1.959734672976055e-04	\\	\hline
			\footnotesize	9	&	\footnotesize	3.000261381083893	&	\footnotesize	1.306851183500157e-04	\\	\hline
			\footnotesize	10	&	\footnotesize	3.000174327057305	&	\footnotesize	8.705402658826245e-05	\\	\hline
			\footnotesize	11	&	\footnotesize	3.000116433888668	&	\footnotesize	5.789316863724636e-05	\\	\hline
			\footnotesize	12	&	\footnotesize	3.000076956251177	&	\footnotesize	3.947763749057032e-05	\\	\hline
			\footnotesize	13	&	\footnotesize	3.000053363008282	&	\footnotesize	2.359324289535891e-05	\\	\hline
			\footnotesize	14	&	\footnotesize	3.000035897764118	&	\footnotesize	1.746524416423867e-05	\\	\hline
			\footnotesize	15	&	\footnotesize	3.000028546336471	&	\footnotesize	7.351427647073194e-06	\\	\hline
			\footnotesize	16	&	\footnotesize	3.000002389324648	&	\footnotesize	2.615701182318020e-05	\\	\hline
			\footnotesize	17	&	\footnotesize	3.000831576720999	&	\footnotesize	8.291873963517382e-04	\\	\hline
			\footnotesize	18	&	\footnotesize	3.000554470371992	&	\footnotesize	2.771063490074610e-04	\\	\hline
			\footnotesize	19	&	\footnotesize	3.000369712689852	&	\footnotesize	1.847576821396579e-04	\\	\hline
			\footnotesize	20	&	\footnotesize	3.000246556354566	&	\footnotesize	1.231563352859233e-04	\\	\hline
			\footnotesize	21	&	\footnotesize	3.000164218617164	&	\footnotesize	8.233773740240480e-05	\\	\hline
			\footnotesize	22	&	\footnotesize	3.000109602674366	&	\footnotesize	5.461594279765336e-05	\\	\hline
			\footnotesize	23	&	\footnotesize	3.000073526984020	&	\footnotesize	3.607569034658198e-05	\\	\hline
			\footnotesize	24	&	\footnotesize	3.000046805273550	&	\footnotesize	2.672171046969041e-05	\\	\hline
			\footnotesize	25	&	\footnotesize	3.000033832581067	&	\footnotesize	1.297269248246025e-05	\\	\hline
			\footnotesize	26	&	\footnotesize	3.000031763455728	&	\footnotesize	2.069125339509981e-06	\\	\hline
			\footnotesize	27	&	\footnotesize	3.000027068531880	&	\footnotesize	4.694923848358457e-06	\\	\hline
			\footnotesize	28	&	\footnotesize	3.000010906132086	&	\footnotesize	1.616239979318479e-05	\\	\hline
			\end{tabular}
		\end{subtable}
		\ifnum\webview=1
			\centering
			\begin{subtable}[t]{.8\textwidth}
		\else
			\begin{subtable}[t]{.49\linewidth}
		\fi
			\centering
			\caption{Improved}
			\begin{tabular}{|c|c|}
			\hline
			\(x_n\)				&	\(E_n=|x_n-x_{n-1}|\)	\\	\hline
			\footnotesize	2.999869302453229	&	\footnotesize	0.020130697546771	\\	\hline
			\footnotesize	2.999997846418646	&	\footnotesize	1.285439654177267e-04	\\	\hline
			\footnotesize	2.983092132985680	&	\footnotesize	0.016905713432966	\\	\hline
			\footnotesize	2.999903080879523	&	\footnotesize	0.016810947893843	\\	\hline
			\footnotesize	2.999999154222051	&	\footnotesize	9.607334252725863e-05	\\	\hline
			\footnotesize	2.921051830315649	&	\footnotesize	0.078947323906402	\\	\hline
			\footnotesize	2.997750655427219	&	\footnotesize	0.076698825111570	\\	\hline
			\footnotesize	2.999998308704638	&	\footnotesize	0.002247653277418	\\	\hline
			\footnotesize	2.992547985979292	&	\footnotesize	0.007450322725346	\\	\hline
			\footnotesize	2.999981350990154	&	\footnotesize	0.007433365010863	\\	\hline
			\footnotesize	3.000103925190753	&	\footnotesize	1.225742005983577e-04	\\	\hline
			\footnotesize	3.000003933963204	&	\footnotesize	9.999122754900114e-05	\\	\hline
			\end{tabular}
		\end{subtable}
		\caption{Comparison of Newton's Method on Iteration at a Root of Order 3}
		\label{newtoncompare3}
	\end{table}
	Obviously, the improved method which only have simple roots converges much faster.
	\end{proof}
\end{enumerate}


% %	sin(x^3)	m=1
% \begin{table}[htbp]
% 	\centering
% 	\begin{tabular}{|c|c|c|}
% 	\hline
% 	$n$	&	\(x_n\)				&	\(E_n=x_n-x_{n-1}\)	\\	\hline
% 	1	&	0.480864091781699	&	0.519135908218301	\\	\hline
% 	2	&	0.319912215554577	&	0.160951876227122	\\	\hline
% 	3	&	0.213236689788164	&	0.106675525766413	\\	\hline
% 	4	&	0.142155565745277	&	0.071081124042887	\\	\hline
% 	5	&	0.094770246815029	&	0.047385318930248	\\	\hline
% 	6	&	0.063180156914457	&	0.031590089900572	\\	\hline
% 	\end{tabular}
% 	\caption{Finding Root of \(\sin\left(x^3\right)\) with regular Newtons Iteration}
% \end{table}



\section{Problem with root finding methods}
Provide an example, if exists, of root-finding problems that satisfy the following criteria:
\begin{enumerate}
	\item It can be solved by Bisection but not by using fixed-point iteration.
	\begin{proof}[Answer]
		By previous result, we only need the function to have Lipschitz constant \(c>1\).
		A typical example is
		\[ f(x)=2|x|. \]
	\end{proof}
	\item It can be solved by using fixed-point iteration but not by Newton's method.
	\begin{proof}[Answer]
		Fixed-point iteration requires Lipschitz continuity, while Newton's method requires differentiable.
		So we can construct a piecewise Lipschitz continuous function
		\[ f(x)=\begin{cases}
		\frac{\left| x\right| }{2} & |x|\leq \frac{1}{2} \\
		\frac{1}{4} &  \frac{1}{2}\leq |x| \leq 1 \\
		\frac{\left| x\right| }{2}-\frac{1}{4} & |x|\geq 1
		\end{cases} \]
		Such function has Lipschitz constant \(c=\frac{1}{2}\) on \(\R\), so it will be solved by fixed-point iteration at any initial guess.
		However, if the initial guess \(x_0\) has \(|x_0|>1\), then
		\[ x_1=x_0-\frac{\frac{|x_0|}{2}-\frac{1}{4}}{\text{sgn}{(x_0)}\cdot\frac{1}{2}}=\frac{1}{2}\cdot\text{sgn}{(x_0)} \]
		But the function is not differentiable at \(\pm\frac{1}{2}\), so the process cannot continue.
		If the initial guess has \(\frac{1}{2}<|x_0|<1\), then \(f'(x_0)=0\), and the process cannot continue either.
		At critical points \(|x_0|=1\) or \(|x_0|=\frac{1}{2}\), the function is not differentiable.
	\end{proof}
\end{enumerate}


\section{Complex root finding for analytic functions}
Let \(f:\C\to\C\) be analytic.
The following iteration formula can be used to solve \(f(z)=0\).
\[ z_{k+1}=z_k-\frac{f(z_k)}{f'(z_k)-\frac{f''(z_k)}{2f'(x_k)}\cdot f(z_k)} \]
where \(f'(z)\) and \(f''(z)\) are the usual complex derivatives of \(f(z)\).
\begin{enumerate}
	\item For the following function
	\[ f(z)=z^7-1 \]
	depict the relationship between the initial point \(z_0=x_0+i y_0\) and the number of iteration required for the sequence to be considered to have converged, \textit{i.e.},
	\[ |z_k - z_{k-1}|^2 < \varepsilon \quad \text{where } \varepsilon=0.0001. \]
	\item What happens to the relationship in part (a) if we use the criterion 
	\[ \left| |z_k|^2 - |z_{k-1}|^2\right| < \varepsilon \quad \text{where } \varepsilon=0.0001. \]
\end{enumerate}