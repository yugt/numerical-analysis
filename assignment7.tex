%!TEX root = main.tex
\titleformat{\section}{\normalfont\large\bfseries}{Exercise \thesection\ --- }{1pt}{}
% \setcounter{chapter}{6}
\renewcommand{\chaptername}{Assignment}
\chapter{Iteration Methods}


\section{Lipschitz continuity vs. Differentiability}
A function \(f:\R\to\R\) is differentiable in some open interval \(\mathcal{I}\subset\R\) if it is differentiable at every point of \(\mathcal{I}\), and it is Lipschitz continuous if there is a constant \(c\geq0\) such that
\[ |f(x_1)-f(x_2)\leq c|x_1-x_2| \qquad \forall x_1,x_2\in\mathcal{I}. \]
\begin{enumerate}
	\item Give a function, with proof, that is differentiable but its derivative is not bounded in some open interval.
	\begin{proof}
	\[ f(x)=\tan x, \quad x\in\left(-\frac{\pi}{2},\frac{\pi}{2}\right) \]
	is differentiable since its derivative is \(f'(x)=\sec^2x\), which is not bounded on its domain.
	\end{proof}
	\item Suppose $f$ is differentiable and $f'$ is bounded in some open interval.
	Prove that $f$ is Lipschitz continuous in that interval.
	\begin{proof}
	We first introduce Cauchy's Mean Value Theorem,
	\begin{theorem}[Mean Value Theorem]
	\label{cauchymean}
	If a function $f$ is continuous on the closed interval \([a,b]\), and differentiable on the open interval \((a,b)\), then there exists a point \(\xi\in(a,b)\) such that
	\[ f'(\xi)=\frac{f(b)-f(a)}{b-a}. \]
	\end{theorem}
	\begin{proof}
	Define \(g(x)=f(x)-rx\), where $r$ is a constant.
	Since $f$ is continuous on \([a,b]\) and {}differentiable on \((a,b)\), the same is true for $g$.
	We now want to choose $r$
	\begin{align*}
	g(a)=g(b)&\iff f(a)-ra=f(b)-rb\\
	&\iff r(b-a)=f(b)-f(a) \\
	&\iff r=\frac{f(b)-f(a)}{b-a}.
	\end{align*}
	By Rolle's theorem \ref{rolle}, since $g$ is differentiable and \(g(a)=g(b)\), there is some \(\xi\in(a,b)\) for which \(g'(\xi)=0\) and thus 
	\[ f'(\xi)=g'(\xi)+r=\frac{f(b)-f(a)}{b-a}\]
	as required.
	\end{proof}
	Then, we can set the Lipschitz constant to be
	\[ c=\sup_{\xi\in(a,b)}|f'(\xi)|. \]
	Since $f'$ is bounded on \((a,b)\), the Lipschitz constant is finite.
	Apply Cauchy's Mean Value Theorem, we get
	\[ \forall x_1,x_2\in(a,b),\quad |f(x_1)-f(x_2)|=|f'(\xi)|\cdot|x_1-x_2|\leq c\cdot|x_1-x_2|, \]
	which means $f$ is Lipschitz continuous.
	\end{proof}
	\item Give a function, with proof, that is differentiable but not Lipschitz continuous in some open interval.
	\begin{proof}
	This is exactly the same as differentiable but the derivative not bounded.	
	\[ f(x)=\sqrt[3]{x}, \quad x\in(-1,1) \]
	is differentiable since its derivative is \(f'(x)=\frac{1}{3}x^{-\frac{2}{3}}\).
	But it is not Lipschitz continuous since
	\[ \frac{|f(x_1)-f(x_2)|}{|x_1-x_2|}=\frac{1}{x_1^{\frac{2}{3}}+x_1^{\frac{1}{3}}x_2^{\frac{1}{3}}+x_2^{\frac{2}{3}}} \]
	is not bounded on \(x_1, x_2\in(-1,1)\) so that the Lipschitz constant does not exist.
	\end{proof}
	\item Give a function, with proof, that is Lipschitz continuous but not differentiable in some open interval.
	\begin{proof}
	\[ f(x)=|x|, \quad x\in(-1,1) \]
	is Lipschitz continuous with Lipschitz constant
	\[ c=\limsup_{x_1,x_2\in(-1,1)}\left|\frac{f(x_1)-f(x_2)}{x_1-x_2}\right|=1. \]
	But it is not differentiable at \(x=0\).
	\end{proof}
\end{enumerate}


\section{Fixed point iteration convergence condition}
A fixed point of a function \(g(x)\) is a real number \(x^*\) such that
\[ g(x^* )=x^*. \]
Assume the followings:
\begin{enumerate}
	\item The function $g$ and its derivative $g'$ are continuous on \([a, b]\), \textit{i.e.}, \(g,g'\in\mathcal{C}[a,b]\).
	\item The function $g$ is bounded below by $a$ and above by $b$, \textit{i.e.}, \(\forall x\in[a,b],g(x)\in[a, b]\).
	\item The initial point \(x_0\) is an interior point of \([a,b]\), \textit{i.e.}, \(x_0\in(a, b)\).
\end{enumerate}
Show the followings are true.
\begin{enumerate}
	\item If \(0\leq|g'(x)|<1, \forall x\in[a,b]\), the fixed-point iteration will converge to the unique fixed point \(x^*\in[a,b]\).
	\begin{proof}
	Define the Lipschitz constant to be
	\[ c=\sup_{\xi\in(a,b)} g'(\xi). \]
	It comes from definition that \(0\leq c<1\).
	Define \(h(x)=g(x)-x\), and it is monotonically decreasing since
	\[ h'(x)=g'(x)-1\in(-2,0), \quad \forall x\in[a,b]. \]
	Thus, \(h(x)=0\) has at most one solution, and so is \(g(x)=x\).
	This proves the uniqueness of \(x^*\).
	Then apply Cauchy's Mean Value Theorem \ref{cauchymean},
	\[ |g(x_{k+1})-g(x_k)|=|g'(\xi)|\cdot|x_{k+1}-x_k|\leq c\cdot|x_{k+1}-x_k|. \]
	Apply the iteration relation,
	\[ x_{k+1}=g(x_k), \qquad x_k=g(x_{x-1}), \]
	we get
	\[ |g(x_{k+1})-g(x_k)|\leq c\cdot|g(x_k)-g(x_{k-1})|. \]
	Such a recursive relation deduces
	\[ |g(x_{k+1})-g(x_k)|\leq c^k\cdot|g(x_1)-g(x_0)|. \]
	Notice that \(g(x)\) is bounded on \([a,b]\), so \(|g(x_1)-g(x_0)|\) is finite, and with \(0\leq c<1\),
	\[ \lim_{k\to\infty}|g(x_{k+1})-g(x_k)|\leq |g(x_1)-g(x_0)|\cdot\lim_{k\to\infty}c^k=0. \]
	Furthermore, the iteration relation also implies
	\[ \lim_{k\to\infty}(x_{k+2}-x_{k+1})=0. \]
	Together with the boundedness of \(x_n\) and \(g\), we conclude that the sequence \(x_n\) is convergent to some \(x^*\in[a,b]\).
	\end{proof}
	\item If \(|g'(x)|>1, \forall x\in[a,b]\), then the fixed-point iteration will never converge to \(x^*\).
	\begin{proof}
	Since \(g'(x)\) is continuous on the interval \([a,b]\),
	\[ |g'(x)|>1 \implies (\forall x\in[a,b], g'(x)>1) \quad \text{or} \quad (\forall x\in[a,b], g'(x)<-1) \]
	So we discuss over these two cases.
	\begin{itemize}
		\item{\(g'(x)>1\):} define \(h(x)=g(x)-x\), and it is monotonically increasing since
		\[ h'(x)=g'(x)-1>0. \]
		Thus, \(h'(x)=0\) has at most one solution, and so is \(g(x)=x\).
		\item{\(g'(x)<-1\):} define \(h(x)=g(x)+x\), and it is monotonically decreasing since
		\[ h'(x)=g'(x)+1<0. \]
		Thus, \(h'(x)=0\) has at most one solution, and so is \(g(x)=x\).
	\end{itemize}
	In both cases, if the solution \(x^*\) exists, then it is unique.
	Define the Lipschitz constant to be
	\[ c=\inf_{\xi\in(a,b)} |g'(\xi)|. \]
	It comes from definition that \(c>1\).
	Then apply Cauchy's Mean Value Theorem \ref{cauchymean},
	\[ |g(x_{k+1})-g(x_k)|=|g'(\xi)|\cdot|x_{k+1}-x_k|\geq c\cdot|x_{k+1}-x_k|. \]
	Apply the iteration relation,
	\[ x_{k+1}=g(x_k), \qquad x_k=g(x_{x-1}), \]
	we get
	\[ |g(x_{k+1})-g(x_k)|\geq c\cdot|g(x_k)-g(x_{k-1})|. \]
	Such a recursive relation deduces
	\[ |g(x_{k+1})-g(x_k)|\geq c^k\cdot|g(x_1)-g(x_0)|=c^{k+1}|x_1-x_0|. \]
	Since \(x^*\) is unique, if \(x_0\neq x^*\), then \(x_1=g(x_0)\neq x_0\).
	Then, with \(c>1\),
	\[ \lim_{k\to\infty}|g(x_{k+1})-g(x_k)|\geq |x_1-x_0|\lim_{k\to\infty} c^{k+1}=+\infty \]
	which implies that the iteration does not converge.
	\end{proof}
\end{enumerate}

\section{Root finding}
Choose a suitable numerical method to find the smallest positive root and the second smallest positive root of the equation
\[ \tan x = 4x \]
correct to 3 decimal places.
Explain your choice.
\begin{proof}[Answer]
Obviously, Newton's method converges quadratically, which is faster than bisection method.
\end{proof}

\section{Order and rate of convergence}
Suppose that the sequence \(\{a_n\}\) converges to the number $L$, and there is a constant \(0<\lambda<\infty\) such that
\[ \lim_{n\to\infty} \frac{|a_{n+1}-L|}{|a_n-L|^\alpha}=\lambda, \]
then the sequence is said to converge to $L$ with \emph{order of convergence} $\alpha$.
The constant $\lambda$ is called the \emph{asymptotic error}.
A positive sequence \(\{E_n\}\) is said to has an order of at least $\alpha$ and a \emph{rate} of at most $\lambda$ if there is a sequence \(\{a_n\}\) that has an order $\alpha$ and a rate of $\lambda$ such that
\[ E_n\leq a_n \quad \forall n\in\N^*. \]
\begin{enumerate}
	\item Find the order of convergence and the rate of convergence of the sequence
	\[ a_n=\frac{1}{n} \quad \forall n\geq1 \]
	\begin{proof}[Answer]
	First, we evaluate the sequence limit.
	\[ L=\lim_{n\to\infty}\frac{1}{n}=0. \]
	Then,
	\[ \lim_{n\to\infty}\frac{|a_{n+1}}{|a_n|}=1 \]
	So the order of the convergence is 1, and the rate of the convergence is also 1.
	\end{proof}
	\item Examine the order of convergence and the rate of convergence of \(\{b_n\}\) where
	\[ b_{2k}=\frac{1}{\ln k} \quad \text{and} \quad b_{2k+1}=\frac{1}{k} \quad \forall k\geq1. \]
	\begin{proof}[Answer]
	First, we evaluate the sequence limit.
	\[ L=\lim_{n\to\infty}\frac{1}{\ln n}=0. \]
	Then,
	\[ \lim_{n\to\infty}\frac{|a_{n+1}}{|a_n|}=1 \]
	So the order of the convergence is 1, and the rate of the convergence is also 1.
	\end{proof}
	\item Use the precise definition above to show the method of fixed-point iteration leads an error sequence that has at least linear convergence.
	\item Find the asymptotic error constant for the method of fixed-point iteration when it has at least quadratic convergence.
	\item Show the order of convergence of the secant method is \(\frac{\sqrt{5}+1}{2} \).
\end{enumerate}


\section{Order of roots}
Suppose that \(f(x)\) and its derivatives \(f'(x),\cdots, f^{(m)}(x)\) are defined and continuous on an
interval containing $x^*$, we say that \(f(x)=0\) has a \emph{root of order} $m$ at \(x=x^*\) if and only ifi
\[ f(x^*)=0, f'(x^*)=0, f''(x^*)=0, \cdots, f^{(m-1)}(x^*)=0, \text{ and } f^{(m)}(x^*)\neq0. \]
The positive integer $m$ is known as the \emph{multiplicity of the root}.
A root of order \(m=1\) is often called a \emph{simple root}, and if \(m>1\), it is called a \emph{multiple root}.
\begin{enumerate}
	\item Prove by induction that there exists a continuous function \(h(x)\) so that \(f(x)\) can be expressed as the product
	\[ f(x)=(x-x^*)^m h(x), \quad \text{where} \quad h(x^*)\neq0 \]
	if the equation \(f(x)\) has a root of order $m$ at \(x=x^*\).
	\item Show the following modified Newton's iteration will produce a sequence that converges quadratically to $x^*$
	\[ x_k=x_{k-1}-m\cdot\frac{f(x_{k-1})}{f'(x_{k-1})} \]
	if the original Newton's method produces a sequence converges linearly to the root \(x=x^*\) of order \(m>1\).
	\item In practice, it is unlikely that m is known, in those cases the following version of modified Newton's method can be applied to accelerate convergence
	\[ x_k=x_{k-1}-\frac{u(x_{k-1})}{u'(x_{k-1})}, \quad \text{where} \quad u(x)=\frac{f(x)}{f'(x)} \]
	Explain how this helps to speed up convergence.
	\item Zero is a root of multiplicity of 3 for the function
	\[ f(x)=\sin(x^3). \]
	Start with \(x_0=1\) and compute \(x_1, x_2, \cdots, x_6\) first by using the iteration formula in part (b) and then by using the iteration formula in part (c).
	What can you conclude based on the values that you have computed?
	\item Compare Newton's method and the modified version of it in part (c) by finding all the roots of the following polynomial correct to 6 decimal places.
	\[ f(x)=x^5 - 11x^4 + 46x^3 - 90x^2 + 81x - 27 \]
\end{enumerate}



\section{Problem with root finding methods}
Provide an example, if exists, of root-finding problems that satisfy the following criteria:
\begin{enumerate}
	\item It can be solved by Bisection but not by using fixed-point iteration.
	\item It can be solved by using fixed-point iteration but not by Newton's method.
\end{enumerate}


\section{Complex root finding for analytic functions}
Let \(f:\C\to\C\) be analytic.
The following iteration formula can be used to solve \(f(z)=0\).
\[ z_{k+1}=z_k \frac{f(z_k)}{f'(z_k)-\frac{f''(z_k)}{2f'(x_k)}\cdot f(z_k)} \]
where \(f'(z)\) and \(f''(z)\) are the usual complex derivatives of \(f(z)\).
\begin{enumerate}
	\item For the following function
	\[ f(z)=z^7-1 \]
	depict the relationship between the initial point \(z_0=x_0+i y_0\) and the number of iteration required for the sequence to be considered to have converged, \textit{i.e.},
	\[ |z_k - z_{k-1}|^2 < \varepsilon \quad \text{where } \varepsilon=0.0001. \]
	\item What happens to the relationship in part (a) if we use the criterion 
	\[ \left| |z_k|^2 - |z_{k-1}|^2\right| < \varepsilon \quad \text{where } \varepsilon=0.0001. \]
\end{enumerate}
